# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M4w5xEhFKCpOij5etGCWWbKJyrLdmmR4


import streamlit as st

from langchain_community.chains import RetrievalQA

from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
import torch

# -------------------------
# 1. Load Vectorstore
# -------------------------
@st.cache_resource
def load_vectorstore():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
    vectorstore = Chroma(persist_directory="chroma_db", embedding_function=embeddings)
    return vectorstore.as_retriever()

# -------------------------
# 2. Load LLM (Gemma 2B IT)
# -------------------------
@st.cache_resource
def load_llm():
    model_id = "google/gemma-2-2b-it"

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    gen_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=350,
        temperature=0.3,
    )

    return HuggingFacePipeline(pipeline=gen_pipeline)

# -------------------------
# 3. Build RAG Chain
# -------------------------
@st.cache_resource
def create_rag_chain():
    retriever = load_vectorstore()
    llm = load_llm()

    chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff"
    )
    return chain

rag = create_rag_chain()

# -------------------------
# 4. Streamlit UI
# -------------------------
st.set_page_config(page_title="RAG System", layout="wide")
st.title("ðŸ“˜ RAG Question Answering with Gemma 2B")

st.write("Ask questions based on your uploaded documents.")

# User input
query = st.text_input("Enter your question")

if st.button("Ask"):
    if query.strip() == "":
        st.warning("Please enter a question.")
    else:
        with st.spinner("Generating response..."):
            response = rag.run(query)
        st.success("Answer:")
        st.write(response)
"""

import streamlit as st

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings

from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_core.prompts import ChatPromptTemplate

from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
import torch


# -------------------------
# 1. Load Vectorstore
# -------------------------
@st.cache_resource
def load_vectorstore():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
    vectorstore = Chroma(persist_directory="chroma_db", embedding_function=embeddings)
    return vectorstore.as_retriever()


# -------------------------
# 2. Load LLM (Gemma 2B IT)
# -------------------------
@st.cache_resource
def load_llm():
    model_id = "google/gemma-2-2b-it"

    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    gen_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=350,
        temperature=0.3,
    )

    return HuggingFacePipeline(pipeline=gen_pipeline)


# -------------------------
# 3. Build RAG Chain (New API)
# -------------------------
@st.cache_resource
def create_rag_chain():
    retriever = load_vectorstore()
    llm = load_llm()

    prompt = ChatPromptTemplate.from_template(
        """
        You are a helpful assistant. Use the context to answer the question.

        Context:
        {context}

        Question:
        {input}
        """
    )

    document_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, document_chain)
    return rag_chain


rag = create_rag_chain()


# -------------------------
# 4. Streamlit App UI
# -------------------------
st.set_page_config(page_title="RAG System", layout="wide")
st.title("ðŸ“˜ RAG Question Answering with Gemma 2B")

query = st.text_input("Enter your question")

if st.button("Ask"):
    if not query.strip():
        st.warning("Please enter a question!")
    else:
        with st.spinner("Thinking..."):
            result = rag.invoke({"input": query})
        
        st.write("### âœ… Answer:")
        st.write(result["answer"])
